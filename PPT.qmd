---
title: "Tarea 3"
subtitle: "Tópicos aplicados en estadística"
author: 
  - "Nicolás Morales"
  - "Eduardo Vásquez"
format: revealjs
editor: visual
---

## Cargar datos

```{python}
import pandas as pd
import numpy as np
 
# To remove the scientific notation from numpy arrays
np.set_printoptions(suppress=True)
 
Data_TR = pd.read_csv("data/train.csv")
Data_TE = pd.read_csv("data/test.csv")

# Contar los NA
summary = Data_TR.describe().transpose()
Data_TR.shape[0] - summary[summary["count"] < Data_TR.shape[0]]["count"]

```

```{python}
import seaborn as sb
import matplotlib.pyplot as plt
for variable in Data_TR.columns:
  try:
    plt.clf()
    sb.boxplot( data = Data_TR[variable], orient="h")
    plt.set(title=variable)
    plt.show()
  except ValueError:
    continue
  
```

```{python}
no_num = Data_TR.select_dtypes(include=['object']).columns.tolist()

# for variable in no_num:
# print(variable, len(Data_TR[variable].unique()))
list(set(Data_TR.columns) - set(Data_TE.columns))
  
no_sirven = ['Name','Nationality','Club','Club_Joining','Birth_Date']
no_sirven.append('National_Kit')    # Demasiados NA's
no_sirven.append('Country_Club_id') # Innecesaria
no_test   = ['National_Position','Country_Club','Club_Kit']
categorias = list(set(no_num) - set(no_sirven))
Y = Data_TR['Club_Position']      # Guardar variable respuesta
no_sirven.append('Club_Position') # Y la quito de las covariables


Data_TR = Data_TR.drop(labels = no_sirven, axis = 1)
Data_TR = Data_TR.drop(labels = no_test  , axis = 1)
X = Data_TR
X = pd.get_dummies(X)#, drop_first = True)
X.shape
```

```{python}
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = to_categorical(encoded_Y)
```

```{python}
# # multi-class classification with Keras
# 
# from keras.models import Sequential
# from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
# 
# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import KFold
# from sklearn.pipeline import Pipeline
# 
# # define baseline model
# def baseline_model():
# 	# create model
# 	model = Sequential()
# 	model.add(Dense(16, input_shape=(X.shape[1],), activation='relu'))
# 	model.add(Dense(29, activation='sigmoid'))
# 	# Compile model
# 	model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])
# 	return model
#  
# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
# kfold = KFold(n_splits=10, shuffle=True)
# results = cross_val_score(estimator, X, dummy_y, cv=kfold)
# print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
```

```{python}
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = pd.DataFrame(sc.fit_transform(X))
```

```{python}
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
model = Sequential()
model.add(Dense(16, input_shape=(X.shape[1],), activation='relu'))
model.add(Dense(29, activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])
model.fit(X, Y, epochs = 10, batch_size = 2)
```

```{python}
Data_TE = Data_TE.drop(labels = ['Nationality', 'Birth_Date'], axis = 1)
Data_TE = pd.get_dummies(Data_TE, drop_first = True)
predictions = model.predict(Data_TE)
```

```{python}
from keras.utils.np_utils import to_categorical
prediction_ = np.argmax(to_categorical(predictions), axis = 1)
prediction_ = encoder.inverse_transform(prediction_)
```

```{python}
from sklearn.neural_network import MLPClassifier
nn = MLPClassifier(hidden_layer_sizes=(2,1), activation='logistic',solver='sgd',learning_rate_init=0.2)
nn.fit(X,Y)
pred=nn.predict(Data_TE)
```

```{python}
from sklearn.model_selection import GridSearchCV
param_grid = {
    'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)],
    'max_iter': [50, 100, 150],
    'activation': ['tanh', 'logistic'],
    'learning_rate': ['constant','adaptive'],
}
grid = GridSearchCV(nn, param_grid, cv=5)

grid.fit(X, Y)
print(grid.best_params_) 
```
